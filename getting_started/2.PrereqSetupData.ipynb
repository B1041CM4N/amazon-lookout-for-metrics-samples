{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisite: Set-up S3 bucket\n",
    "\n",
    "In this notebook, we are going to use the provided e-commerce dataset to demonstrate the functionality of Amazon Lookout For Metrics. This process is meant to be educational and to guide you through similar approaches you can use on your own datasets.\n",
    "\n",
    "## Data set-up workflow:\n",
    "\n",
    "1. Create a bucket\n",
    "2. Uncompress dataset\n",
    "3. Save data to bucket\n",
    "\n",
    "After these steps have been completed you are ready to get started exploring the data with Amazon Lookout For Metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import boto3\n",
    "import utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create bucket\n",
    "\n",
    "As mentioned above, data needs to exist somewhere. Let's run the next cell to create a bucket for you to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "s3_bucket = account_id + \"-lookoutmetricscf\"\n",
    "\n",
    "region = \"us-west-2\"\n",
    "utility.create_bucket(s3_bucket, region=region)\n",
    "\n",
    "s3_bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncompress dataset\n",
    "\n",
    "Let's uncompress the provided e-commerce data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dirname = os.path.join(\"./data\")\n",
    "\n",
    "if os.path.exists(data_dirname):\n",
    "    shutil.rmtree(data_dirname)\n",
    "os.makedirs(data_dirname)\n",
    "\n",
    "zip_filename = os.path.join(\"./ecommerce.zip\")\n",
    "\n",
    "with zipfile.ZipFile( zip_filename, \"r\" ) as zip_fd:\n",
    "    zip_fd.extractall(data_dirname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed to the next step let's take a quick look at the folder structure. Specifically, notice that we only have one `input.csv` in the `backtest`. Whereas, data in the `live` folder is broken down into days (ex: `20201001` for October 1, 2020) and hours (ex: `0200` for 2:00AM)\n",
    "\n",
    "Also, notice that our data goes far into the future. Of course, this is unrealistic of any real-world scenarios but it works for this demontration. You can check the last date to ensure it is still in the future. If it is not, please open an issue up on this project's Github page immediately! Thank you for catching it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "paths = utility.DisplayablePath.make_tree(pathlib.Path('data'))\n",
    "for path in paths:\n",
    "    print(path.displayable())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when you take a quick look into the data, you will notice the schema for `backtest` and `live` data is identical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_df = pd.read_csv('data/ecommerce/backtest/input.csv')\n",
    "backtest_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "live_sample_df = pd.read_csv('data/ecommerce/live/20201208/0000/20201208_000000.csv')\n",
    "live_sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data to bucket\n",
    "\n",
    "Finally, let save the data into to our s3 bucket. Note the `--quiet` at the end of the command, this will prevent the output from consuming a ton of resources in this browser window(you'd see thousands of files listed here without it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 sync {data_dirname}/ecommerce/ s3://{s3_bucket}/ecommerce/ --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things easier on yourself we are going to leverage the magic functions of Ipython in order to save a few variables for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store s3_bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now your data has been loaded into S3 and you can continue on to the continous test via `3.GettingStartedWithLiveData.ipynb`. After that you can explore backtesting while you wait for anomalies to be detected by opening `4.BacktestingWithHistoricalData.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

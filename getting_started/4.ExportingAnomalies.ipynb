{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting Anomalies\n",
    "\n",
    "No matter if you have chosen a backtesting project or a continous Detector, you may want to export the findings from the service into either a Pandas dataframe or a simple CSV file for usage in other tools or systems. This notebook will walk you through the process of connecting to a Detector, querying for anomalies, and writing them into a format for usage later. Simply run the cells below after updating your `Detector_ARN` value with the corresponding one from your account and you're all set.\n",
    "\n",
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import zipfile\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import utility\n",
    "import synth_data\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookoutmetrics_client = boto3.client( \"lookoutmetrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Detector_ARN = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you ahve specified your Detector's ARN the next task is to inspect it and learn about the frequency it uses, this is critical later to ensuring we build the exported file correctly. The cell below does just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = lookoutmetrics_client.describe_anomaly_detector(AnomalyDetectorArn=Detector_ARN)\n",
    "\n",
    "anomaly_detector_frequency = response[\"AnomalyDetectorConfig\"][\"AnomalyDetectorFrequency\"]\n",
    "if anomaly_detector_frequency==\"PT5M\":\n",
    "    frequency = \"5Min\"\n",
    "    frequency_timedelta = datetime.timedelta(minutes=5)\n",
    "elif anomaly_detector_frequency==\"PT10M\":\n",
    "    frequency = \"10Min\"\n",
    "    frequency_timedelta = datetime.timedelta(minutes=10)\n",
    "elif anomaly_detector_frequency==\"PT1H\":\n",
    "    frequency = \"1H\"\n",
    "    frequency_timedelta = datetime.timedelta(hours=1)\n",
    "elif anomaly_detector_frequency==\"P1D\":\n",
    "    frequency = \"1D\"\n",
    "    frequency_timedelta = datetime.timedelta(days=1)\n",
    "else:\n",
    "    assert False, \"unknown frequency\" + anomaly_detector_frequency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Anomalies \n",
    "\n",
    "Next up we need to loop over all the anomaly groups that have been collected and build them into a list to parse later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_groups = []\n",
    "next_token = None\n",
    "\n",
    "while True:    \n",
    "    params = {\n",
    "        \"AnomalyDetectorArn\" : Detector_ARN,\n",
    "        \"SensitivityThreshold\" : 0,\n",
    "        \"MaxResults\" : 100,\n",
    "    }\n",
    "\n",
    "    if next_token:\n",
    "        params[\"NextToken\"] = next_token\n",
    "\n",
    "    response = lookoutmetrics_client.list_anomaly_group_summaries( **params )\n",
    "\n",
    "    anomaly_groups += response[\"AnomalyGroupSummaryList\"]\n",
    "\n",
    "    if \"NextToken\" in response:\n",
    "        next_token = response[\"NextToken\"]\n",
    "        continue\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Dataframe\n",
    "\n",
    "At last the time has come to iterate over the results and build a dataframe to house them. Simply run the cell below to get a nice organized collection of your anomalies. Note this cell can take a minute or so to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "dimension_names = set()\n",
    "\n",
    "for anomaly_group in anomaly_groups:\n",
    "\n",
    "    def datetime_from_string(s):\n",
    "        try:\n",
    "            dt = datetime.datetime.fromisoformat(s.split(\"[\")[0])\n",
    "        except ValueError:\n",
    "            dt = datetime.datetime.strptime(s.split(\"[\")[0], \"%Y-%m-%dT%H:%MZ\")\n",
    "\n",
    "        return dt\n",
    "\n",
    "    start_time = datetime_from_string( anomaly_group[\"StartTime\"] )\n",
    "    end_time = datetime_from_string( anomaly_group[\"EndTime\"] )\n",
    "    anomaly_group_id = anomaly_group[\"AnomalyGroupId\"]\n",
    "    anomaly_group_score = anomaly_group[\"AnomalyGroupScore\"]\n",
    "    primary_metric_name = anomaly_group[\"PrimaryMetricName\"]\n",
    "\n",
    "    time_series_list = []\n",
    "    next_token = None\n",
    "\n",
    "    while True:    \n",
    "\n",
    "        params = {\n",
    "            \"AnomalyDetectorArn\" : Detector_ARN,\n",
    "            \"AnomalyGroupId\" : anomaly_group_id,\n",
    "            \"MetricName\" : primary_metric_name,\n",
    "            \"MaxResults\" : 100,\n",
    "        }\n",
    "\n",
    "        if next_token:\n",
    "            params[\"NextToken\"] = next_token\n",
    "\n",
    "        response = lookoutmetrics_client.list_anomaly_group_time_series( **params )\n",
    "\n",
    "        time_series_list += response[\"TimeSeriesList\"]\n",
    "\n",
    "        if \"NextToken\" in response:\n",
    "            next_token = response[\"NextToken\"]\n",
    "            continue\n",
    "\n",
    "        break\n",
    "\n",
    "    for time_series in time_series_list:\n",
    "        data = {}\n",
    "\n",
    "        for dimension in time_series[\"DimensionList\"]:\n",
    "            data[ dimension[\"DimensionName\"]] = [ dimension[\"DimensionValue\"]]\n",
    "            dimension_names.add(dimension[\"DimensionName\"])\n",
    "        data[primary_metric_name + \"_group_score\"] = [anomaly_group_score]\n",
    "\n",
    "        t = start_time\n",
    "        while t<=end_time:\n",
    "            data[ \"timestamp\" ] = [ t ]\n",
    "            df_part = pd.DataFrame(data)\n",
    "            df_list.append(df_part)\n",
    "            t += frequency_timedelta\n",
    "\n",
    "df = pd.concat(df_list)\n",
    "\n",
    "# fold multiple metrics into same rows\n",
    "df = df.groupby([\"timestamp\", *dimension_names], as_index=False).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will render the first few rows of your anomalies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the Results\n",
    "\n",
    "The very last step below will create a CSV file for you to use later, once the file has been created you can right click and download the file out of JupyterLab using the file browser on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = Detector_ARN.split(':')[-1] + \"_anomalies.csv\"\n",
    "df.to_csv(filename, index=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
